{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8d9bf7",
   "metadata": {},
   "source": [
    "# ðŸ”— LangChain + LLM Abstractions\n",
    "\n",
    "In this notebook weâ€™ll move from the raw OpenAI SDK to **LangChain**, which provides higher-level abstractions to make LLM use easier.\n",
    "\n",
    "Weâ€™ll cover:\n",
    "1. Setting up `ChatOpenAI`.  \n",
    "2. Prompt templates & the **LangChain Expression Language (LCEL)**.  \n",
    "3. Varying parameters like `temperature` and `top_p`.  \n",
    "4. **Streaming** responses with callbacks.  \n",
    "5. **Batching & parallel calls** with `.batch()`.  \n",
    "6. Getting **structured outputs** with Pydantic models.  \n",
    "7. Swapping configs at runtime (`.bind`, `.with_config`).  \n",
    "8. (Optional) Using Azure OpenAI through LangChain.\n",
    "\n",
    "By the end, youâ€™ll see how LangChain **simplifies interaction, chaining, and integration** with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c722d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,   # same knobs as OpenAI\n",
    "    # top_p=0.9,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76d8891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here are two quick tips for learning Python:\\n\\n1. **Practice Regularly**: Consistency is key when learning a programming language. Set aside time each day or week to write code, work on small projects, or solve coding challenges on platforms like LeetCode, HackerRank, or Codewars. This will help reinforce your understanding and improve your problem-solving skills.\\n\\n2. **Utilize Online Resources**: Take advantage of the wealth of online tutorials, courses, and documentation available. Websites like Codecademy, freeCodeCamp, and Coursera offer structured learning paths. Additionally, the official Python documentation is an excellent resource for understanding language features and libraries.\\n\\nHappy coding!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 16, 'total_tokens': 154, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CXN9mQgPJ9zAMFsoPNSpLcDrVty7q', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--182a154f-e4ab-4c3a-ae5d-4d3e5cb98d63-0', usage_metadata={'input_tokens': 16, 'output_tokens': 138, 'total_tokens': 154, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Give me two quick tips for learning Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13044ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Please provide the text you'd like me to summarize.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 30, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CXN9tK8A0riS3f4Hw4iFm46vHEBna', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5e690e02-c7b5-4db2-9c3e-f1998ff12b0f-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "        (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "    (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61197895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful, concise assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Summarize this in 2 bullets:\\n\\n{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d77e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = \"Transformers use attention to weigh context; embeddings turn tokens into vectors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c8ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful, concise assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Summarize this in 2 bullets:\\n\\nTransformers use attention to weigh context; embeddings turn tokens into vectors.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_rendered = prompt.invoke({\"text\": text_example})\n",
    "prompt_rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4524de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt_rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39404ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize contextual information in processing data.\n",
      "- Embeddings convert tokens into vector representations for effective computation within the model.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb98571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize contextual information in processing data.\n",
      "- Embeddings convert tokens into vector representations for effective computation within the model.\n"
     ]
    }
   ],
   "source": [
    "content = StrOutputParser().invoke(response)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5667bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb7b0d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Transformers utilize attention mechanisms to prioritize context in processing information.\\n- Embeddings convert tokens into vector representations for better model understanding.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\": text_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b7b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"text\": text_example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2f7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize contextual information in processing data.\n",
      "- Embeddings convert tokens into vector representations for effective input handling in models.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d191f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentle drops descend,  \n",
      "Whispering secrets to earth,  \n",
      "Pavement sparkles bright.  \n",
      "Nature's soft embrace wraps,  \n",
      "Dreams awaken in gray skies.  \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class PrintHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        print(token, end=\"\")\n",
    "\n",
    "stream_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, streaming=True, callbacks=[PrintHandler()])\n",
    "(stream_llm | StrOutputParser()).invoke(\"Stream a 5-sentence haiku about rain.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99161421",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca4ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is overfitting?\",\n",
    "    \"Explain dropout in one line.\",\n",
    "    \"Contrast precision vs recall briefly.\"\n",
    "]\n",
    "# Runnable.batch for parallel execution\n",
    "answers = chain.batch(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14a61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is overfitting?\n",
      "A: Overfitting is a common problem in machine learning and statistical modeling where a model learns the training data too well, capturing noise and fluctuations rather than the underlying patterns. This results in a model that performs exceptionally well on the training dataset but poorly on unseen data or validation sets. \n",
      "\n",
      "Key characteristics of overfitting include:\n",
      "\n",
      "1. **High Training Accuracy, Low Validation/Test Accuracy**: The model shows excellent performance on the training data but fails to generalize to new, unseen data.\n",
      "\n",
      "2. **Complex Models**: Overfitting often occurs with overly complex models that have too many parameters relative to the amount of training data. These models can fit the training data very closely, including its noise.\n",
      "\n",
      "3. **Insufficient Data**: When the training dataset is small or not representative of the broader population, the model may latch onto specific patterns that do not generalize.\n",
      "\n",
      "To mitigate overfitting, several techniques can be employed:\n",
      "\n",
      "- **Cross-Validation**: Using techniques like k-fold cross-validation helps ensure that the model's performance is evaluated on multiple subsets of the data.\n",
      "- **Regularization**: Adding penalties for larger coefficients in models (like L1 or L2 regularization) can help constrain the model's complexity.\n",
      "- **Pruning**: In decision trees, pruning can reduce the size of the tree by removing sections that provide little power in predicting target variables.\n",
      "- **Early Stopping**: Monitoring the model's performance on a validation set and stopping training when performance starts to degrade can prevent overfitting.\n",
      "- **Data Augmentation**: Increasing the amount of training data through techniques like augmentation can help the model learn more generalized patterns.\n",
      "\n",
      "In summary, overfitting is a critical issue in model training that can lead to poor performance on new data, and it is important to implement strategies to avoid it.\n",
      "\n",
      "Q: Explain dropout in one line.\n",
      "A: Dropout is a regularization technique in neural networks that randomly sets a fraction of the neurons to zero during training to prevent overfitting.\n",
      "\n",
      "Q: Contrast precision vs recall briefly.\n",
      "A: Precision and recall are two important metrics used to evaluate the performance of classification models, particularly in scenarios where class distribution is imbalanced.\n",
      "\n",
      "- **Precision** measures the accuracy of the positive predictions made by the model. It is defined as the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). High precision indicates that when the model predicts a positive class, it is often correct.\n",
      "\n",
      "  \\[\n",
      "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
      "  \\]\n",
      "\n",
      "- **Recall** (also known as sensitivity or true positive rate) measures the model's ability to identify all relevant instances. It is defined as the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). High recall indicates that the model successfully captures most of the positive cases.\n",
      "\n",
      "  \\[\n",
      "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
      "  \\]\n",
      "\n",
      "In summary, precision focuses on the quality of positive predictions, while recall emphasizes the model's ability to find all positive instances. Balancing these two metrics is crucial, especially in applications where the cost of false positives and false negatives varies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, a in zip(questions, answers):\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23501a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flashcard(term='Positional Encoding', definition='A technique used in Transformers to inject information about the position of tokens in a sequence, enabling the model to understand the order of words.')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Flashcard(BaseModel):\n",
    "    term: str = Field(..., description=\"Short term\")\n",
    "    definition: str = Field(..., description=\"One-sentence definition\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Flashcard)  # Let LC coax JSONâ†’model\n",
    "card = structured_llm.invoke(\"Create a flashcard about 'positional encoding' in Transformers.\")\n",
    "card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff2a4b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positional Encoding'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
